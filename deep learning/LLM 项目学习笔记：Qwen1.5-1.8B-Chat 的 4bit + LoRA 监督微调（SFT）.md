
> 目标：用小规模对话数据，把通用聊天基座微调成“更像某个朋友/人设”的聊天风格，并用可复现的方式做对比评估。

> 本仓库相关脚本：训练 `llm.py`、数据清洗 `clear_data.py`、聊天推理 `chat.py`、单人对比 `compare.py`、多人对比 `multi_compare.py`。

> 运行产物（可重复生成）：`compare_results.md`（base vs 单个 LoRA），`multi_compare_results.md`（base vs 3 personas）。报告顶部包含本次生成时间与生成参数，方便复现实验。
---
## 1 技术路线总览（做什么 & 为什么这么做）

这套方案可以理解为一条“性价比优先”的微调流水线：

1. **SFT（监督微调）**：用成对的「用户输入 → 助手输出」让模型学习“怎么回答”和“以什么语气回答”。

2. **LoRA（低秩适配）**：不改动全量参数，只训练一小部分“可插拔”的适配器权重，把训练成本和保存体积大幅压缩。

3. **4bit 量化加载（nf4）**：训练/加载时把大模型权重压到 4bit，进一步降低显存压力（配合 bf16/fp16 计算）。

最终你保存的不是完整基座权重，而是：

- LoRA 适配器（`adapter_model.safetensors` + `adapter_config.json`）

- tokenizer / chat template（用于保证训练与推理提示词格式一致）

推理时：基座模型 + 某个 LoRA 适配器 组合在一起得到最终风格。

---
## 2 实现思路（从数据到模型的完整闭环）

### 2.1 数据如何塑造“人设/风格”

SFT 的本质是“模仿分布”：

- 数据里出现的口癖、句式、情绪倾向，会被模型学到并放大。

- 数据里出现的噪声（空回复、乱码、截断、无意义短答），也会直接体现在生成上（例如回答过短、答非所问）。

因此，数据侧通常要保证两件事：

- 结构正确：每条样本都能稳定映射为「system/user/assistant」对话格式。

- 质量可控：对坏样本做过滤，对过短/空洞输出做约束或补充。

仓库里数据采用 JSONL：每行一个对象，核心字段是 `instruction` 和 `output`。

**学习补充：什么是“人设建模”（persona modeling）？**

这里的“人设”不是指真实身份，而是指模型在对话中的可控行为特征。在工程上通常可以拆成三层：

1. **角色约束（System）**：你希望它扮演什么（朋友/导师/吐槽搭子/客服），以及边界是什么（不说脏话、不编造、先问清楚再建议等）。

2. **语气与表达（Style）**：口头禅、长度偏好（短句/长答）、是否爱分点、是否更共情/更犀利、是否喜欢反问等。

3. **内容偏好（Preference）**：面对常见问题的倾向性（先安慰再建议/先给步骤/先总结），以及常出现的话题域（学习/考研/生活琐事）。

SFT 会把这些“层”的统计规律一起学进去，因此你能看到 tuned 模型更像某种口吻；但同时也要警惕：**如果训练集中“短答/省略号/无信息回复”占比高，模型就会把“信息量不足”也当成风格的一部分**。

实操建议：

- 想让 persona“稳”：保持 system 约束固定、训练样本风格一致、减少互相冲突的例子。

- 想让 persona“有用”：确保输出里不仅有语气词，也有可执行信息（例如 1~2 个具体动作或一句具体解释）。

- 多 persona：优先用“一个 persona 一个 LoRA”分开训练（便于切换与对比），而不是把多风格混在同一适配器里。
### 2.2 Prompt / Chat Template：训练与推理必须同构

“模板一致”是风格微调是否有效的关键：

- 训练时：system + user + assistant（assistant 给出标准答案）

- 推理时：system + user +（assistant 作为 generation 起点）

如果模板不一致，常见表现是：

- 微调“像没生效”（风格不明显）

- 角色不稳定（system 约束不起作用）

- 把用户话当成要生成的内容（结构错位）

本项目通过 tokenizer 的 `apply_chat_template` 来统一模板，避免手写 prompt 时的细节偏差。

**学习补充：为什么“Chat 一致性”这么关键？**

可以把 SFT 看成在学习一个条件分布：`P(assistant | system, user, history)`。模板不一致，本质是你在训练时和推理时给模型看的“条件变量”不一样，导致：

- 训练学到的是 A 格式下怎么回答，推理却用 B 格式提问 → 风格不显著或行为异常。

- system 约束在训练里存在、推理里弱化/丢失 → 人设不稳定。

- role 标签错位（把 assistant 当 user）→ 可能出现“复读用户内容/自问自答”等结构性错误。

为了让“同构”更直观，你可以用这个抽象结构理解（不绑定任何具体实现）：

- 训练样本输入：`[System: 规则与人设] + [User: 问题]`

- 训练目标输出：`[Assistant: 期望回答]`

- 推理时输入：`[System: 规则与人设] + [User: 问题] + [Assistant: <开始生成>]`

因此你在整个链路里要做的是：保证同一份 tokenizer/chat template 负责把消息结构转换为模型可理解的 prompt，而不是训练用一套、推理用另一套。
### 2.3 LoRA：改动最小但影响最大的“关键层”

LoRA 通常注入在注意力投影与 MLP 投影层，因为它们对：

- 语气风格（语调、口癖、礼貌程度）

- 组织方式（长短、分点、解释倾向）

影响非常明显，而参数量又可控。

实践上需要关注两点：

- **模块名是否匹配**（不同模型架构命名不同；不匹配就等于没注入）

- **LoRA 容量与过拟合**（`r` 越大越能拟合风格，也更容易把训练集口癖“背下来”）

**学习补充：LoRA 的原理

LoRA 的核心思想是：不直接更新大模型某个线性层的完整权重矩阵 `W`，而是在该层旁路加入一个低秩增量：

- 原始：`y = xW`

- LoRA：`y = x(W + ΔW)`，其中 `ΔW = A · B`，并且 `A`、`B` 的秩很低（`rank=r`）

训练时只更新 `A/B`（以及少量相关参数），因此：

- **训练更省**：需要反传与保存的参数量显著减少。

- **部署更灵活**：同一个基座模型上可以加载多个适配器（persona 可插拔）。

- **风格更好调**：很多“表达方式”的变化不需要改动全量权重。

为什么 LoRA 特别适合做 persona？

- persona 更像“输出分布的偏置/倾向”而非“新增知识库”。

- 低秩增量往往足以改变语言风格、长度偏好、组织结构，而不必重塑整个模型能力。

常见 trade-off（概念层面）：

- `r`/适配器容量越大：越能贴合风格，也越容易过拟合（比如回答变短、口癖变重、复读训练集套路）。

- 数据越少：越应重视“样本质量”和“system 约束”，而不是一味加大 LoRA 容量或训练轮数。

### 2.4 4bit + 分页优化器：让单卡4060也能跑得动

对于 1~3B 级别模型，常见瓶颈是显存：

- 4bit 量化用于“存储侧降本”

- bf16/fp16 计算用于“算力侧折中”

- 分页优化器用于降低优化器状态占用

获得的是：更低门槛的本地训练/复现实验能力，但代价是：

- 平台兼容性（Windows 的 bitsandbytes 可能更敏感）

- 数值行为更依赖版本组合（transformers/trl/peft/bnb）
---
## 3 训练与推理：工程化落地怎么做

### 3.1 训练流水线（概念步骤）

1. 数据清洗（过滤坏样本，保证结构一致）

2. 加载基座模型（可选 4bit 量化以省显存）

3. 注入 LoRA（只训练适配器参数）

4. 用 SFT 跑训练（注意模板、学习率、epoch，避免过拟合）

5. 导出 LoRA 适配器与 tokenizer（形成可插拔的 persona）

### 3.2 推理与“记忆”的实现思路

`chat.py` 的“记忆”属于最简单可控的一类：**把最近 N 轮对话直接拼到上下文**。

- 优点：实现简单，可解释，便于复现。

- 局限：上下文线性增长，长对话会截断；只按“最近”取，不按“相关”取。

升级方向（只写思路）：

- **摘要记忆**：把更早对话压缩成 summary，保留长期信息。

- **检索记忆**：embedding + top-k 取相关历史，再与近期对话拼接。
---
## 4 评估：怎么证明“微调确实改变了模型”

评估最怕“凭感觉”。这里建议遵循三条原则：

1. 固定 prompts：覆盖情绪、建议、吐槽、简短回复等场景。

2. 固定生成参数：温度、top_p、最大长度等都固定，才可对比。

3. 同屏对照：base vs tuned 并排展示，差异才清晰。

本仓库已把结果写成可阅读的 Markdown 报告（建议直接打开看全量）：

- base vs 三个 persona：`multi_compare_results.md`
### 4.1 让输出更“像人类”：长度与结构的控制思路

若“tuned 太短、不像人类”在 persona 微调里很常见，原因通常是：训练集中短答/语气词比例偏高，模型就会把“短”也学成风格。

实践上可以从三层入手（不依赖改代码细节）：

1. **提示词层**：在 prompt 里明确要求“写成一段话、3-5 句话、不列点/不编号”，并避免再加“xx字以内”这类硬性短答约束。

2. **解码层**：不要全局强行 `min_new_tokens`（容易跑飞/重复）；更稳的做法是“先正常生成，如果过短再让它展开一次”。

3. **数据层**：补充“口语但信息密度高”的样本（共情 + 1-2 个可执行动作），把“像朋友聊天”与“有用”同时学进去。
### 4.2 节选：base vs 单个 LoRA

下面只放几条最能体现差异的节选（完整请看 `compare_results.md`）。

#### 4.2.1 Prompt：我今天又拖延了，一下午啥也没干，心态有点崩。

**Base（微调前，节选）**

```

听到你最近感到拖延和心态崩溃，我非常理解你的感受。

...

1. 设定明确的目标：...

2. 制定计划：...

3. 避免分心：...

```
**Tuned（微调后）**

```

好吧，给你写一下。其实还是没有完全删除，现在觉得可以稍作调整，但到底是什么地方，我觉得还是以后再改好了，现阶段更改可能只是破坏原有的和谐感。……（存在即合理，或许）

```
### 4.3 节选：三人 LoRA（base vs girl/jiangjian/peixuan）

仓库里当前可见三份风格数据与对应 LoRA：

- `girl`：`girl_cleaned.jsonl` + `girl_qwen_chat_1.8b/`

- `jiangjian`：`jiang_jian_style.jsonl` + `jiangjian_qwen_chat_1.8b/`

- `peixuan`：`peixuan.jsonl` + `peixuan_qwen_chat_1.8b/`

完整对比请看 `multi_compare_results.md`。这里节选一条：
#### 4.3.1 Prompt：我准备考研但很焦虑。给我一个今晚能做的最小行动，再说一句鼓励的话。

**Base**
```

今晚可以做的一件小事是：睡前半小时做一次“最小复习”，比如只看一页笔记或只做一题，做完就停。然后给自己一句鼓励：焦虑很正常，但你今天愿意动一下就已经在变好了。……（完整见报告）

```
**girl**

```

躺到现阶段状态能接受的最好的作息时间。

```
**jiangjian**

```

背一个今天不会的单词。

```
**peixuan**

```

备考只是一个过程，而不是终点，别拿它跟前程比。只要每天有目标，每天都努力一点点，坚持下来就水到渠成了。……（完整见报告）

```
---
## 5 常见问题与改进方向（只讲思路）

### 5.1 tuned 回答过短/空洞

原因往往来自数据分布：训练集中短答比例高，或者输出包含大量“语气词/省略号”。

改进思路：

- 数据侧补充“高信息密度”的示例

- 训练侧降低 epoch / 调整学习率，缓解背训练集口癖

- 推理侧加入长度/结构约束（例如要求“用 2~3 句回答”）

### 5.2 风格不稳定 / 像没微调

优先检查“模板一致性”：训练时和推理时的 system/user/assistant 结构是否同构，tokenizer 的 chat template 是否一致加载。
### 5.3 多 persona 的工程化

多 persona 的关键是“可插拔”：

- 每个 persona 一套 LoRA adapter

- 同一基座模型上加载多个 adapter，通过切换 adapter 选择风格

这也是 `multi_compare.py` 的实现思路：同一 base + 多个 adapter 做并列对比。