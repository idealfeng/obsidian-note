
> 目标：用小规模对话数据，把通用聊天基座微调成“更像某个朋友/人设”的聊天风格，并用可复现的方式做对比评估。

> 本仓库相关脚本：训练 `llm.py`、数据清洗 `clear_data.py`、聊天推理 `chat.py`、单人对比 `compare.py`、多人对比 `multi_compare.py`。

> 运行产物（可重复生成）：`compare_results.md`（base vs 单个 LoRA），`multi_compare_results.md`（base vs 3 personas）。报告顶部包含本次生成时间与生成参数，方便复现实验。
---
## 1 技术路线总览（做什么 & 为什么这么做）

这套方案可以理解为一条“性价比优先”的微调流水线：

1. **SFT（监督微调）**：用成对的「用户输入 → 助手输出」让模型学习“怎么回答”和“以什么语气回答”。

2. **LoRA（低秩适配）**：不改动全量参数，只训练一小部分“可插拔”的适配器权重，把训练成本和保存体积大幅压缩。

3. **4bit 量化加载（nf4）**：训练/加载时把大模型权重压到 4bit，进一步降低显存压力（配合 bf16/fp16 计算）。

最终你保存的不是完整基座权重，而是：

- LoRA 适配器（`adapter_model.safetensors` + `adapter_config.json`）

- tokenizer / chat template（用于保证训练与推理提示词格式一致）

推理时：基座模型 + 某个 LoRA 适配器 组合在一起得到最终风格。

---
## 2 实现思路（从数据到模型的完整闭环）

### 2.1 数据如何塑造“人设/风格”

SFT 的本质是“模仿分布”：

- 数据里出现的口癖、句式、情绪倾向，会被模型学到并放大。

- 数据里出现的噪声（空回复、乱码、截断、无意义短答），也会直接体现在生成上（例如回答过短、答非所问）。

因此，数据侧通常要保证两件事：

- 结构正确：每条样本都能稳定映射为「system/user/assistant」对话格式。

- 质量可控：对坏样本做过滤，对过短/空洞输出做约束或补充。

仓库里数据采用 JSONL：每行一个对象，核心字段是 `instruction` 和 `output`。

**学习补充：什么是“人设建模”（persona modeling）？**

这里的“人设”不是指真实身份，而是指模型在对话中的可控行为特征。在工程上通常可以拆成三层：

1. **角色约束（System）**：你希望它扮演什么（朋友/导师/吐槽搭子/客服），以及边界是什么（不说脏话、不编造、先问清楚再建议等）。

2. **语气与表达（Style）**：口头禅、长度偏好（短句/长答）、是否爱分点、是否更共情/更犀利、是否喜欢反问等。

3. **内容偏好（Preference）**：面对常见问题的倾向性（先安慰再建议/先给步骤/先总结），以及常出现的话题域（学习/考研/生活琐事）。

SFT 会把这些“层”的统计规律一起学进去，因此你能看到 tuned 模型更像某种口吻；但同时也要警惕：**如果训练集中“短答/省略号/无信息回复”占比高，模型就会把“信息量不足”也当成风格的一部分**。

实操建议：

- 想让 persona“稳”：保持 system 约束固定、训练样本风格一致、减少互相冲突的例子。

- 想让 persona“有用”：确保输出里不仅有语气词，也有可执行信息（例如 1~2 个具体动作或一句具体解释）。

- 多 persona：优先用“一个 persona 一个 LoRA”分开训练（便于切换与对比），而不是把多风格混在同一适配器里。
### 2.2 Prompt / Chat Template：训练与推理必须同构

“模板一致”是风格微调是否有效的关键：

- 训练时：system + user + assistant（assistant 给出标准答案）

- 推理时：system + user +（assistant 作为 generation 起点）

如果模板不一致，常见表现是：

- 微调“像没生效”（风格不明显）

- 角色不稳定（system 约束不起作用）

- 把用户话当成要生成的内容（结构错位）

本项目通过 tokenizer 的 `apply_chat_template` 来统一模板，避免手写 prompt 时的细节偏差。

**学习补充：为什么“Chat 一致性”这么关键？**

可以把 SFT 看成在学习一个条件分布：`P(assistant | system, user, history)`。模板不一致，本质是你在训练时和推理时给模型看的“条件变量”不一样，导致：

- 训练学到的是 A 格式下怎么回答，推理却用 B 格式提问 → 风格不显著或行为异常。

- system 约束在训练里存在、推理里弱化/丢失 → 人设不稳定。

- role 标签错位（把 assistant 当 user）→ 可能出现“复读用户内容/自问自答”等结构性错误。

为了让“同构”更直观，你可以用这个抽象结构理解（不绑定任何具体实现）：

- 训练样本输入：`[System: 规则与人设] + [User: 问题]`

- 训练目标输出：`[Assistant: 期望回答]`

- 推理时输入：`[System: 规则与人设] + [User: 问题] + [Assistant: <开始生成>]`

因此你在整个链路里要做的是：保证同一份 tokenizer/chat template 负责把消息结构转换为模型可理解的 prompt，而不是训练用一套、推理用另一套。
### 2.3 LoRA：改动最小但影响最大的“关键层”

LoRA 通常注入在注意力投影与 MLP 投影层，因为它们对：

- 语气风格（语调、口癖、礼貌程度）

- 组织方式（长短、分点、解释倾向）

影响非常明显，而参数量又可控。

实践上需要关注两点：

- **模块名是否匹配**（不同模型架构命名不同；不匹配就等于没注入）

- **LoRA 容量与过拟合**（`r` 越大越能拟合风格，也更容易把训练集口癖“背下来”）

**学习补充：LoRA 的原理

LoRA 的核心思想是：不直接更新大模型某个线性层的完整权重矩阵 `W`，而是在该层旁路加入一个低秩增量：

- 原始：`y = xW`

- LoRA：`y = x(W + ΔW)`，其中 `ΔW = A · B`，并且 `A`、`B` 的秩很低（`rank=r`）

训练时只更新 `A/B`（以及少量相关参数），因此：

- **训练更省**：需要反传与保存的参数量显著减少。

- **部署更灵活**：同一个基座模型上可以加载多个适配器（persona 可插拔）。

- **风格更好调**：很多“表达方式”的变化不需要改动全量权重。

为什么 LoRA 特别适合做 persona？

- persona 更像“输出分布的偏置/倾向”而非“新增知识库”。

- 低秩增量往往足以改变语言风格、长度偏好、组织结构，而不必重塑整个模型能力。

常见 trade-off（概念层面）：

- `r`/适配器容量越大：越能贴合风格，也越容易过拟合（比如回答变短、口癖变重、复读训练集套路）。

- 数据越少：越应重视“样本质量”和“system 约束”，而不是一味加大 LoRA 容量或训练轮数。

### 2.4 4bit + 分页优化器：让单卡4060也能跑得动

对于 1~3B 级别模型，常见瓶颈是显存：

- 4bit 量化用于“存储侧降本”

- bf16/fp16 计算用于“算力侧折中”

- 分页优化器用于降低优化器状态占用

获得的是：更低门槛的本地训练/复现实验能力，但代价是：

- 平台兼容性（Windows 的 bitsandbytes 可能更敏感）

- 数值行为更依赖版本组合（transformers/trl/peft/bnb）
---
## 3 训练与推理：工程化落地怎么做

### 3.1 训练流水线（概念步骤）

1. 数据清洗（过滤坏样本，保证结构一致）

2. 加载基座模型（可选 4bit 量化以省显存）

3. 注入 LoRA（只训练适配器参数）

4. 用 SFT 跑训练（注意模板、学习率、epoch，避免过拟合）

5. 导出 LoRA 适配器与 tokenizer（形成可插拔的 persona）

### 3.2 推理与“记忆”的实现思路

`chat.py` 的“记忆”属于最简单可控的一类：**把最近 N 轮对话直接拼到上下文**。

- 优点：实现简单，可解释，便于复现。

- 局限：上下文线性增长，长对话会截断；只按“最近”取，不按“相关”取。

升级方向（只写思路）：

- **摘要记忆**：把更早对话压缩成 summary，保留长期信息。

- **检索记忆**：embedding + top-k 取相关历史，再与近期对话拼接。
---
## 4 评估：怎么证明“微调确实改变了模型”

评估最怕“凭感觉”。这里建议遵循三条原则：

1. 固定 prompts：覆盖情绪、建议、吐槽、简短回复等场景。

2. 固定生成参数：温度、top_p、最大长度等都固定，才可对比。

3. 同屏对照：base vs tuned 并排展示，差异才清晰。

本仓库已把结果写成可阅读的 Markdown 报告（建议直接打开看全量）：

- base vs 三个 persona：`multi_compare_results.md`
### 4.1 让输出更“像人类”：长度与结构的控制思路

若“tuned 太短、不像人类”在 persona 微调里很常见，原因通常是：训练集中短答/语气词比例偏高，模型就会把“短”也学成风格。

实践上可以从三层入手（不依赖改代码细节）：

1. **提示词层**：在 prompt 里明确要求“写成一段话、3-5 句话、不列点/不编号”，并避免再加“xx字以内”这类硬性短答约束。

2. **解码层**：不要全局强行 `min_new_tokens`（容易跑飞/重复）；更稳的做法是“先正常生成，如果过短再让它展开一次”。

3. **数据层**：补充“口语但信息密度高”的样本（共情 + 1-2 个可执行动作），把“像朋友聊天”与“有用”同时学进去。

**补充：为什么“长度差不多了，但对不上题/跑题”也常见？**

这通常也和**数据量小 + 风格样本偏“自说自话/省略**有关：LoRA 很容易学到口癖与句式，却不一定学到“严格按指令解决问题”的习惯；当测试 prompt 与训练分布差异较大时，就会出现“看得出口吻变了，但回答没对齐需求”。

### 4.2 节选：base vs 单个 LoRA

下面只放几条最能体现差异的节选。

#### 4.2.1 Prompt：我刚刷手机刷到现在了……救救我，怎么收手？给我一个“现在就做”的动作。

**Base（微调前，节选）**

```

最近我经常熬夜刷手机，导致白天精神状态不佳，工作效率低下。我非常想要控制住这种情况，但总是无法做到。共情： 你可能感到疲惫不堪，身心俱疲，每天都在为工作和生活奔波而感到压力山大。这种长时间的屏幕依赖不仅影响你的身体健康，还可能对你的心理健康产生负面影响，如焦虑、抑郁等情绪问题。现在就做的小动作： 首先，你可以尝试将手机放在离床一臂远的地方，避免在床上看手机。

```

**Tuned（微调后）**

```

我一般自己设定多久看一次手机为限，基本不超过两次。我之前有个朋友也是差不多，不过她家很严控这个，连看手机的时间都不算。。我以前也经常会因为一些事熬夜，所以有时候会有点反差感，但其实也只是单纯觉得和人比起来，我比较幸运，有睡眠。我今天早上八点起的，虽然还是有点想睡，但是已经不似昨天凌晨一点才睡那么痛苦了。

```

> 读法：base 更像“标准助手/说明文 + 结构化建议”；tuned 更像“自述式/朋友口吻”，但可执行动作可能不如 base 明确——这就是“风格学到了，但指令对齐程度未必同步提升”的典型现象。

### 4.3 节选：三人 LoRA（base vs girl/jiangjian/peixuan）

仓库里当前可见三份风格数据与对应 LoRA：

- `girl`：`girl_cleaned.jsonl` + `girl_qwen_chat_1.8b/`

- `jiangjian`：`jiang_jian_style.jsonl` + `jiangjian_qwen_chat_1.8b/`

- `peixuan`：`peixuan.jsonl` + `peixuan_qwen_chat_1.8b/`

#### 4.3.1 Prompt 1：我准备考研但很焦虑。给我一个今晚能做的最小行动，再说一句鼓励的话。

**关键词：** `考研`、`焦虑`、`今晚`

---

Base（微调前）

`我正面临研究生入学考试的紧张压力，心情起伏不定，感到焦虑和不安。我想要尽快缓解这种情绪，以下是一些今晚可以实施的小行动：  首先，我会花一些时间进行深呼吸和冥想，以帮助放松身心并降低焦虑感。我会找一个安静的地方坐下，闭上眼睛，专注于自己的呼吸，感受空气进入肺部，然后慢慢呼出，重复这个过程几次。这可以帮助我调整心态，减轻对考试结果的担忧。  其次，我会制定一份详细的学习计划，并为每个科目设定明确的目标和期限。这样可以让我有条不紊地安排时间和精力，避免因为临时抱佛脚而过度投入，从而增加焦虑感。同时，我会尽量将学习内容分解成小块，每天分配一定的时间来复习和练习，逐步提高知识掌握程度。  此外，我也打算在睡前进行适量的运动，如散步或者瑜伽，以帮助改善睡眠质量，减少因焦虑导致的身体不适。这些轻度的运动不仅可以释放身体中的紧张情绪，还可以帮助我在第二天保持清醒和专注。  最后，我想向您表达我对你的深深支持和鼓励。我知道面对这样的挑战`

---

girl（微调后）

`你其实已经很努力了，只是有时候会看不见自己很棒。加油！（虽然我觉得你根本不用听这番）`

---

jiangjian（微调后）

`我的专业是计算机，现在大三了。刚学的时候觉得很难，但是越学越有动力。现在每天都会花一个小时做题，虽然还是很难，但是比刚开始好多了。今天又做了两个小时，感觉有很大进步。明天开始系统复习，应该能考上。`

---

peixuan（微调后）

`先规划一下夜宵，吃顿饭然后躺下放松一下`

#### 4.3.2 Prompt 2：我一直刷手机停不下来。给我一个现在就能执行的动作，并说明为什么有效。

**关键词：** `刷手机`、`停不下来`、`现在`

---

Base（微调前）

`用户：最近我发现自己无法控制自己刷手机的习惯，总是在睡前忍不住看手机，导致睡眠质量下降，白天也容易感到疲惫和焦虑。  共情：  理解你的困扰，这确实是一种常见的问题，许多人都有类似的经历。过度依赖手机往往会导致我们忽视了生活中的其他重要事情，如与家人、朋友的交流，以及处理日常生活琐事的能力。这种行为不仅影响到我们的身心健康，还可能对我们的社交技能和社会适应能力产生负面影响。  具体可执行建议：  1. **设定明确的睡前时间**：在睡觉前一小时停止所有电子设备的使用，包括手机、电脑等。这样可以帮助大脑逐渐放松并进入深度睡眠状态。  2. **制定实际目标**：为自己设定一些具体的、可以衡量的小目标，如每天只看一定数量的社交媒体或阅读一定量的文章。当这些目标达成时，你会更有动力不再沉迷于手机。  3. **寻找替代活动**：找到一些可以代替手机的活动，如冥想、听音乐、阅读书籍或者进行户外运动等。这些活动不仅可以帮助你在睡前保持放松，还可以提高你的注意力和集中力。  4.`

---

girl（微调后）

`（其实还好吧，我是个事事求全的人，对现状要求过高，所以……）`

---

jiangjian（微调后）

`我的情况是：大学期间每天晚上十一点前要把一天学的内容复习一遍。因为这个习惯让我有成就感，所以一直坚持下来了`

---

peixuan（微调后）

`那你每天看多长时间`

---
## 8 常见问题与改进方向（只讲思路）

### 8.1 tuned 回答过短/空洞

原因往往来自数据分布：训练集中短答比例高，或者输出包含大量“语气词/省略号”。

改进思路：

- 数据侧补充“高信息密度”的示例

- 训练侧降低 epoch / 调整学习率，缓解背训练集口癖

- 推理侧加入长度/结构约束（例如要求“用 2~3 句回答”）

### 8.2 风格不稳定 / 像没微调

优先检查“模板一致性”：训练时和推理时的 system/user/assistant 结构是否同构，tokenizer 的 chat template 是否一致加载。
### 8.3 多 persona 的工程化

多 persona 的关键是“可插拔”：

- 每个 persona 一套 LoRA adapter

- 同一基座模型上加载多个 adapter，通过切换 adapter 选择风格
