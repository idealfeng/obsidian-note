
这是一套“从 0 到能写小爬虫”的练习集合：从 **发请求**（urllib/requests）到 **解析提取**（XPath/BeautifulSoup/JsonPath），再到 **动态页面**（Selenium）与 **工程化**（Scrapy）。

---
## 1 重点

  
1. **接口优先**：能抓 JSON 接口就别抓 HTML。  

2. **先定位数据源**：页面显示 ≠ 数据来源（可能是接口/渲染后 DOM/二次请求）。  

3. **抽取要可维护**：选择器越“稳定”，后续维护成本越低（id/结构化接口 > 复杂 xpath）。  

4. **先稳再快**：超时、重试、限速、日志这些比“并发”更重要。  

5. **请求要像工程**：每个请求都要有 `timeout`，错误要可追踪。  

6. **分页/增量是常态**：设计时就考虑翻页、去重、断点续跑。  

7. **登录态最容易过时**：cookie/验证码/签名参数只学流程，不死磕细节。  

8. **动态页面先抓接口**：用浏览器跑起来只是手段，目标是找到稳定数据源。  

9. **数据落地要规范**：优先 JSON Lines/CSV，字段命名统一、可重复运行不乱。  

10. **合规第一**：robots/条款/版权/隐私红线别碰，访问频率要克制。

---
## 2 选型思路（决策链）

1) **能抓接口就别抓页面**：DevTools 找到 JSON 接口 → requests/Scrapy 直接抓（最稳、最快）  
2) **没有接口就抓 HTML**：拿到 HTML → XPath/bs4 提取  

3) **HTML 没数据/必须交互**：Selenium（后续可升级 Playwright）把页面跑起来，再解析或转抓接口  

4) **要长期跑/要规模化**：上 Scrapy（并发、调度、管道、扩展点更完整）

> 仓库里有 cookie 登录/验证码/签名参数等示例，这类内容非常容易随网站更新而失效：建议学流程与方法，不死磕可运行性。

---
## 3 实战流程（从“想抓”到“可交付”）

把一次抓取任务拆成 6 步，会非常稳：

1. **定义目标**：要什么字段？范围多大？更新频率？允许的延迟？  

2. **找数据源**：优先接口；其次 HTML；最后才是浏览器自动化。  

3. **设计抓取策略**：分页/并发/限速/重试/断点续跑/去重。  

4. **抽取与清洗**：字段对齐、缺失处理、类型转换、统一编码。  

5. **落地与校验**：存储格式、抽样校验、错误样本留存（便于回归）。  

6. **监控与维护**：日志、失败重跑、选择器/接口变更快速定位。

---
## 4 你会学到什么（能力清单）

- HTTP 基础：URL 参数、请求头、GET/POST、状态码、编码、异常

- 解析与抽取：HTML（XPath/CSS）、JSON（字段提取/JsonPath）、JSONP 去壳

- 稳定性与反爬常识：限速、重试、会话（cookie/session）、代理（入门）

- 动态页面：定位、交互、等待、无头浏览器、截图

- Scrapy 工程化：Spider / Item / Pipeline / Settings、分页、二段式抓取、CrawlSpider + Rule
---
## 5 仓库结构（按学习模块组织）

- `爬虫/`：单文件脚本（urllib/requests/解析/selenium 等）

  - 本地样例：`爬虫/bs4_test.html`、`爬虫/urllib_解析_path.html`、`爬虫/jsonpath_本地文件.json`

- `爬虫/scrapy_baidu/`：Scrapy 多 spider + pipeline（含图片下载 demo）

- `爬虫/scrapy_dushu/`：Scrapy `CrawlSpider + Rule + LinkExtractor` demo

- `爬虫/scrapy_movie/`：Scrapy 列表页 → 详情页（二段式）demo
仓库已提供 `.gitignore`，会忽略常见缓存/生成物（如 `__pycache__`、`*.pyc`、部分 demo 输出、Scrapy 下载目录等）。
---
## 6 如何快速“找到接口”（非常关键）

1. 打开浏览器 DevTools → Network  

2. 勾选 Preserve log，清空记录  

3. 在页面上执行一次你关心的动作（翻页/搜索/筛选）  

4. 在 Network 里筛 `fetch/xhr`，优先找返回 JSON 的请求  

5. 点开 Response/Preview，确认字段就是你要的  

6. 复制请求：URL、方法、headers（尤其 Referer）、payload、分页参数

判断接口是否更稳的小技巧：

- 返回 JSON 且字段结构清晰：通常比 HTML 稳  

- URL/参数看起来像业务接口（非一次性 hash）：通常更稳  

- 不依赖复杂签名/加密：维护成本更低（学习阶段优先选）
---
## 7 稳定性 Checklist（让脚本“能跑很久”）

建议把这些当默认配置：

- **超时**：每次请求都设置超时（连接超时 + 读取超时）  

- **重试**：对网络错误/5xx 做有限次重试（建议指数退避）  

- **限速**：全局 QPS 控制；遇到 429/403 直接降速  

- **错误留存**：保存失败 URL + 状态码 + 响应片段（便于复现）  

- **断点续跑**：把“已抓过”的 key 存起来（ID/URL hash）  

- **输出幂等**：同一输入重复跑，不应无限追加垃圾数据  

- **日志分级**：info 看进度，warning 看异常，error 看失败样本

> 一句话：先把“失败可定位、可重跑”做好，再谈更高并发。
---
## 8 常见坑（看懂就够）

- **Cookie/验证码/签名参数**：极易过时；学习阶段优先掌握“定位数据源 + 正确抽取 + 稳定性”

- **`urlretrieve` 下载视频**：视频网站通常不是直链；需要解析真实媒体地址/鉴权（示例可一笔带过）

- **requests 参数位置**：推荐显式使用 `params=`（GET）和 `data=`（POST），避免位置参数传错

- **302/跳验证页**：通常是风控/反爬；先降速、换数据源（接口/公开页），别硬刚

- **乱码**：先确认响应编码/解码方式，保存原始响应用于排查

---
## 9 未来展望（方向）

- 动态站点：从 Selenium 迁移到 Playwright（自动等待、网络拦截、多上下文更舒服）

- 性能与并发：了解 `asyncio + httpx/aiohttp`，但建议先把 Scrapy 用熟

- 数据工程：增量更新、去重、质量校验、调度（Cron/Airflow/K8s CronJob）、监控告警

- 抽取方式：规则（XPath/CSS）为主，必要时再考虑引入 LLM 做结构化抽取加速
